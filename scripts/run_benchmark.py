"""TTS ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸.

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” BenchmarkRunnerë¥¼ ì‚¬ìš©í•˜ì—¬
TTS ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    python scripts/run_benchmark.py
    python scripts/run_benchmark.py --device cpu --iterations 3
    python scripts/run_benchmark.py --models gtts kokoro
"""

import sys
import argparse
from pathlib import Path
from typing import List, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tests.test_runner import BenchmarkRunner
from utils.performance_monitor import PerformanceMonitor


def parse_arguments() -> argparse.Namespace:
    """ì»¤ë§¨ë“œ ë¼ì¸ ì¸ìë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    
    Returns:
        íŒŒì‹±ëœ ì¸ìë“¤
    """
    parser = argparse.ArgumentParser(
        description='TTS ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‹¤í–‰ (ëª¨ë“  ëª¨ë¸, auto ë””ë°”ì´ìŠ¤, 5íšŒ ë°˜ë³µ)
  python scripts/run_benchmark.py
  
  # CPUì—ì„œ 3íšŒ ë°˜ë³µ
  python scripts/run_benchmark.py --device cpu --iterations 3
  
  # íŠ¹ì • ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸
  python scripts/run_benchmark.py --models gtts
  
  # ì—¬ëŸ¬ ëª¨ë¸ ì§€ì •
  python scripts/run_benchmark.py --models gtts kokoro --iterations 10
        """
    )
    
    parser.add_argument(
        '--device',
        type=str,
        choices=['cpu', 'cuda', 'auto'],
        default='auto',
        help='ì‹¤í–‰í•  ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: auto)'
    )
    
    parser.add_argument(
        '--iterations',
        type=int,
        default=5,
        help='ê° í…ŒìŠ¤íŠ¸ì˜ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)'
    )
    
    parser.add_argument(
        '--models',
        type=str,
        nargs='+',
        default=None,
        help='í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: ì „ì²´ ëª¨ë¸)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='config/models_config.yaml',
        help='ëª¨ë¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: config/models_config.yaml)'
    )
    
    parser.add_argument(
        '--skip-confirmation',
        action='store_true',
        help='ì‹œì‘ ì „ í™•ì¸ ë©”ì‹œì§€ ê±´ë„ˆë›°ê¸°'
    )
    
    return parser.parse_args()


def print_header() -> None:
    """í—¤ë”ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "=" * 70)
    print(" " * 20 + "TTS ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ")
    print("=" * 70)


def print_system_info(device: str) -> None:
    """ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
    """
    print("\n[ì‹œìŠ¤í…œ ì •ë³´]")
    monitor = PerformanceMonitor(device=device)
    sys_info = monitor.get_system_info()
    
    print(f"  CPU ì½”ì–´: {sys_info['cpu_count']} (ë¬¼ë¦¬: {sys_info.get('cpu_count_physical', 'N/A')})")
    print(f"  ë©”ëª¨ë¦¬: {sys_info['total_memory_gb']} GB "
          f"(ì‚¬ìš© ê°€ëŠ¥: {sys_info['available_memory_gb']} GB)")
    
    if sys_info.get('gpu_available'):
        print(f"  GPU: {sys_info['gpu_name']}")
        print(f"  GPU ë©”ëª¨ë¦¬: {sys_info['gpu_total_memory_gb']} GB")
        print(f"  CUDA ë²„ì „: {sys_info.get('cuda_version', 'N/A')}")
    else:
        print(f"  GPU: ì‚¬ìš© ì•ˆ í•¨")
    
    print(f"  ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {monitor.device}")


def filter_models(
    runner: BenchmarkRunner,
    selected_models: Optional[List[str]]
) -> None:
    """ì„ íƒëœ ëª¨ë¸ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    
    Args:
        runner: BenchmarkRunner ì¸ìŠ¤í„´ìŠ¤
        selected_models: ì„ íƒëœ ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
    
    Raises:
        ValueError: ì„ íƒëœ ëª¨ë¸ì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
    """
    if selected_models is None:
        return
    
    # ì„ íƒëœ ëª¨ë¸ë§Œ í•„í„°ë§
    available_models = set(runner.models.keys())
    selected_set = set(selected_models)
    
    # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í™•ì¸
    invalid_models = selected_set - available_models
    if invalid_models:
        print(f"\nâš ï¸  ê²½ê³ : ë‹¤ìŒ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {', '.join(invalid_models)}")
        print(f"     ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(available_models)}")
    
    # ìœ íš¨í•œ ëª¨ë¸ë§Œ í•„í„°ë§
    valid_models = selected_set & available_models
    
    if not valid_models:
        raise ValueError(
            f"ì„ íƒëœ ëª¨ë¸ ì¤‘ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\n"
            f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(available_models)}"
        )
    
    runner.models = {k: v for k, v in runner.models.items() if k in valid_models}
    print(f"\nâœ“ ì„ íƒëœ ëª¨ë¸ë¡œ í•„í„°ë§: {', '.join(runner.models.keys())}")


def print_benchmark_config(
    runner: BenchmarkRunner,
    iterations: int,
    device: str
) -> None:
    """ë²¤ì¹˜ë§ˆí¬ ì„¤ì •ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        runner: BenchmarkRunner ì¸ìŠ¤í„´ìŠ¤
        iterations: ë°˜ë³µ íšŸìˆ˜
        device: ë””ë°”ì´ìŠ¤
    """
    total_tests = len(runner.models) * len(runner.test_sentences) * iterations
    
    print("\n" + "-" * 70)
    print("[ë²¤ì¹˜ë§ˆí¬ ì„¤ì •]")
    print(f"  ëª¨ë¸: {', '.join(runner.models.keys())} ({len(runner.models)}ê°œ)")
    print(f"  í…ŒìŠ¤íŠ¸ ë¬¸ì¥: {len(runner.test_sentences)}ê°œ")
    print(f"  ë°˜ë³µ íšŸìˆ˜: {iterations}íšŒ")
    print(f"  ë””ë°”ì´ìŠ¤: {device}")
    print(f"  ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {total_tests}ê°œ")
    
    # ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (ëŒ€ëµì )
    estimated_time = total_tests * 2  # í…ŒìŠ¤íŠ¸ë‹¹ ì•½ 2ì´ˆ ê°€ì •
    minutes = estimated_time // 60
    seconds = estimated_time % 60
    print(f"  ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {minutes}ë¶„ {seconds}ì´ˆ")
    print("-" * 70)


def print_final_summary(runner: BenchmarkRunner) -> None:
    """ìµœì¢… ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        runner: BenchmarkRunner ì¸ìŠ¤í„´ìŠ¤
    """
    if not runner.results:
        print("\nâš ï¸  ì €ì¥ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    import pandas as pd
    df = pd.DataFrame(runner.results)
    
    print("\n" + "=" * 70)
    print(" " * 25 + "ìµœì¢… ìš”ì•½")
    print("=" * 70)
    
    # ì „ì²´ í†µê³„
    print(f"\n[ì „ì²´ í†µê³„]")
    print(f"  ì´ í…ŒìŠ¤íŠ¸: {len(df)}ê°œ")
    print(f"  ì„±ê³µë¥ : 100%")
    print(f"  í‰ê·  RTF: {df['rtf'].mean():.4f} (Â±{df['rtf'].std():.4f})")
    print(f"  í‰ê·  ì¶”ë¡  ì‹œê°„: {df['inference_time'].mean():.3f}ì´ˆ (Â±{df['inference_time'].std():.3f})")
    print(f"  í‰ê·  ë©”ëª¨ë¦¬: {df['peak_memory_mb'].mean():.2f} MB")
    
    # ëª¨ë¸ë³„ ìš”ì•½
    print(f"\n[ëª¨ë¸ë³„ ìš”ì•½]")
    for model_name in df['model'].unique():
        model_df = df[df['model'] == model_name]
        
        avg_rtf = model_df['rtf'].mean()
        avg_time = model_df['inference_time'].mean()
        max_memory = model_df['peak_memory_mb'].max()
        
        # ì„±ëŠ¥ í‰ê°€
        if avg_rtf < 0.5:
            badge = "ğŸ‰ ë§¤ìš° ë¹ ë¦„"
        elif avg_rtf < 1.0:
            badge = "âœ… ë¹ ë¦„"
        else:
            badge = "âš ï¸  ëŠë¦¼"
        
        print(f"\n  [{model_name.upper()}] {badge}")
        print(f"    í…ŒìŠ¤íŠ¸ ìˆ˜: {len(model_df)}ê°œ")
        print(f"    í‰ê·  RTF: {avg_rtf:.4f} (Â±{model_df['rtf'].std():.4f})")
        print(f"    í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.3f}ì´ˆ")
        print(f"    ìµœëŒ€ ë©”ëª¨ë¦¬: {max_memory:.2f} MB")
        print(f"    í‰ê·  ì˜¤ë””ì˜¤ ê¸¸ì´: {model_df['audio_duration'].mean():.2f}ì´ˆ")
    
    # RTF ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    print(f"\n[ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±]")
    rtf_target = 0.5
    models_below_target = []
    
    for model_name in df['model'].unique():
        model_df = df[df['model'] == model_name]
        avg_rtf = model_df['rtf'].mean()
        
        if avg_rtf < rtf_target:
            models_below_target.append(model_name)
            status = "âœ… ë‹¬ì„±"
        else:
            status = "âŒ ë¯¸ë‹¬ì„±"
        
        print(f"  {model_name}: RTF < {rtf_target} â†’ {status} (í‰ê· : {avg_rtf:.4f})")
    
    if models_below_target:
        print(f"\n  ğŸ‰ {len(models_below_target)}ê°œ ëª¨ë¸ì´ ëª©í‘œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
    
    print("\n" + "=" * 70)


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜."""
    args = parse_arguments()
    
    try:
        print_header()
        print_system_info(args.device)
        
        # 1. BenchmarkRunner ì´ˆê¸°í™”
        print("\n" + "=" * 70)
        print("[1ë‹¨ê³„] BenchmarkRunner ì´ˆê¸°í™”")
        print("=" * 70)
        runner = BenchmarkRunner(config_path=args.config)
        
        # 2. ëª¨ë¸ ì´ˆê¸°í™”
        print("\n" + "=" * 70)
        print("[2ë‹¨ê³„] ëª¨ë¸ ì´ˆê¸°í™”")
        print("=" * 70)
        models = runner.initialize_models()
        
        if not models:
            print("\nâŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("   ëª¨ë¸ ë˜í¼ë¥¼ ë¨¼ì € êµ¬í˜„í•˜ì„¸ìš”.")
            sys.exit(1)
        
        # 3. ëª¨ë¸ í•„í„°ë§ (ì„ íƒëœ ëª¨ë¸ë§Œ)
        if args.models:
            print("\n" + "=" * 70)
            print("[3ë‹¨ê³„] ëª¨ë¸ í•„í„°ë§")
            print("=" * 70)
            filter_models(runner, args.models)
        
        # 4. í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¡œë“œ
        print("\n" + "=" * 70)
        print("[4ë‹¨ê³„] í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¡œë“œ")
        print("=" * 70)
        sentences = runner.load_test_sentences()
        
        if not sentences:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        # 5. ë²¤ì¹˜ë§ˆí¬ ì„¤ì • í™•ì¸
        print_benchmark_config(runner, args.iterations, args.device)
        
        # 6. ì‚¬ìš©ì í™•ì¸
        if not args.skip_confirmation:
            print("\nâš ï¸  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ì—ëŠ” ë‹¤ë¥¸ ì‘ì—…ì„ ìµœì†Œí™”í•˜ì„¸ìš”.")
            print("   (ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•´)")
            print()
            response = input("ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? [Y/n]: ")
            
            if response.lower() in ['n', 'no']:
                print("\në²¤ì¹˜ë§ˆí¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                sys.exit(0)
        
        # 7. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        print("\n" + "=" * 70)
        print("[5ë‹¨ê³„] ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
        print("=" * 70)
        runner.run_benchmark(num_iterations=args.iterations)
        
        # 8. ê²°ê³¼ ì €ì¥
        print("\n" + "=" * 70)
        print("[6ë‹¨ê³„] ê²°ê³¼ ì €ì¥")
        print("=" * 70)
        runner.save_results()
        
        # 9. ìµœì¢… ìš”ì•½
        print_final_summary(runner)
        
        # 10. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
        print("\n" + "=" * 70)
        print("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        print("=" * 70)
        print("\n[ìƒì„±ëœ íŒŒì¼]")
        print("  ğŸ“Š results/metrics/benchmark_results_*.csv")
        print("  ğŸ“ˆ results/metrics/summary_statistics_*.csv")
        print("\n[ë‹¤ìŒ ë‹¨ê³„]")
        print("  - ê²°ê³¼ ì‹œê°í™”: python3 scripts/visualize_results.py")
        print("  - ê²°ê³¼ ë¶„ì„: pandasë¡œ CSV íŒŒì¼ ë¶„ì„")
        print("  - ì˜¤ë””ì˜¤ í™•ì¸: data/output/{model_name}/ í´ë”")
        print()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

